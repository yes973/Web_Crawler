{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네이버 Openapi와 urllib, beautifulsoup을 활용해 네이버를 검색했을 때\n",
    "블로그 탭에 노출되는 게시물의 제목, 작성자, 날짜, 내용을 크롤링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nan as NA\n",
    "from pandas import Series,DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "naver_client_id = \"your_id\"\n",
    "naver_client_secret = \"your_secret\"\n",
    "\n",
    "def naver_blog_crawling(search_blog_keyword, display_count, sort_type):\n",
    "    search_result_blog_page_count = get_blog_search_result_pagination_count(search_blog_keyword, display_count)[0]\n",
    "    total_posting_count = get_blog_search_result_pagination_count(search_blog_keyword, display_count)[1]\n",
    "    crawled_dataframe = get_blog_post(search_blog_keyword, display_count, search_result_blog_page_count, sort_type)\n",
    "    return crawled_dataframe\n",
    "\n",
    "def get_blog_search_result_pagination_count(search_blog_keyword, display_count):\n",
    "    encode_search_keyword = urllib.parse.quote(search_blog_keyword)\n",
    "    url = \"https://openapi.naver.com/v1/search/blog?query=\" + encode_search_keyword\n",
    "    request = urllib.request.Request(url)\n",
    "\n",
    "    request.add_header(\"X-Naver-Client-Id\", naver_client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", naver_client_secret)\n",
    "\n",
    "    response = urllib.request.urlopen(request)\n",
    "    response_code = response.getcode()\n",
    "\n",
    "    if response_code is 200:\n",
    "        response_body = response.read()\n",
    "        response_body_dict = json.loads(response_body.decode('utf-8'))\n",
    "\n",
    "        if response_body_dict['total'] == 0:\n",
    "            blog_pagination_count = 0\n",
    "        else:\n",
    "            blog_pagination_total_count = math.ceil(response_body_dict['total'] / int(display_count))\n",
    "\n",
    "            if blog_pagination_total_count >= 1000:\n",
    "                blog_pagination_count = 1000\n",
    "            else:\n",
    "                blog_pagination_count = blog_pagination_total_count\n",
    "\n",
    "        return blog_pagination_count,response_body_dict['total']\n",
    "    \n",
    "def get_blog_post(search_blog_keyword, display_count, search_result_blog_page_count, sort_type):\n",
    "    encode_search_blog_keyword = urllib.parse.quote(search_blog_keyword)\n",
    "\n",
    "    total_posting_count = get_blog_search_result_pagination_count(search_blog_keyword, display_count)[1]\n",
    "    title,postdate,bloggername,text = [],[],[],[]\n",
    "\n",
    "    for i in range(1, 101, display_count):\n",
    "        url = \"https://openapi.naver.com/v1/search/blog?query=\" + encode_search_blog_keyword + \"&display=\" + str(\n",
    "            display_count) + \"&start=\" + str(i) + \"&sort=\" + sort_type\n",
    "\n",
    "        request = urllib.request.Request(url)\n",
    "\n",
    "        request.add_header(\"X-Naver-Client-Id\", naver_client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\", naver_client_secret)\n",
    "\n",
    "        response = urllib.request.urlopen(request)\n",
    "        response_code = response.getcode()\n",
    "\n",
    "        if response_code is 200:\n",
    "            response_body = response.read()\n",
    "            response_body_dict = json.loads(response_body.decode('utf-8'))\n",
    "\n",
    "            for j in range(0, len(response_body_dict['items'])):\n",
    "                blog_post_url = response_body_dict['items'][j]['link'].replace(\"amp;\", \"\")\n",
    "\n",
    "                get_blog_post_content_code = requests.get(blog_post_url)\n",
    "                get_blog_post_content_text = get_blog_post_content_code.text\n",
    "\n",
    "                get_blog_post_content_soup = BeautifulSoup(get_blog_post_content_text, 'lxml')\n",
    "                \n",
    "                df_for_row = pd.DataFrame()\n",
    "\n",
    "                for link in get_blog_post_content_soup.select('#mainFrame'):\n",
    "                    real_blog_post_url = \"http://blog.naver.com\" + link.get('src')\n",
    "\n",
    "                    get_real_blog_post_content_code = requests.get(real_blog_post_url)\n",
    "                    get_real_blog_post_content_text = get_real_blog_post_content_code.text\n",
    "\n",
    "                    get_real_blog_post_content_soup = BeautifulSoup(get_real_blog_post_content_text, 'lxml')\n",
    "\n",
    "                    remove_html_tag = re.compile('<.*?>')\n",
    "                    \n",
    "                    blog_post_title = re.sub(remove_html_tag, '', response_body_dict['items'][j]['title'])\n",
    "                    blog_post_description = re.sub(remove_html_tag, '',\n",
    "                                                   response_body_dict['items'][j]['description'])\n",
    "                    blog_post_postdate = datetime.strptime(response_body_dict['items'][j]['postdate'],\n",
    "                                                                    \"%Y%m%d\").strftime(\"%y.%m.%d\")\n",
    "                    blog_post_blogger_name = response_body_dict['items'][j]['bloggername']\n",
    "                    \n",
    "                    blog_post_content_text=\"\"\n",
    "                    \n",
    "                    for blog_post_content in get_real_blog_post_content_soup.select('.se_textView'):\n",
    "                        blog_post_content_text = blog_post_content_text + blog_post_content.get_text()\n",
    "                    \n",
    "                    title.append(blog_post_title)\n",
    "                    postdate.append(blog_post_postdate)\n",
    "                    bloggername.append(blog_post_blogger_name)\n",
    "                    text.append(blog_post_content_text)\n",
    "                    \n",
    "    return pd.DataFrame({'title' : title, 'postdate' : postdate, 'bloggername' : bloggername, 'text' : text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 검색어, 크롤링할 페이지수, 검색결과 정렬순서를 지정해 데이터프레임 형식으로 추출할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = naver_blog_crawling('웹 크롤링', 10, 'sim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>postdate</th>\n",
       "      <th>bloggername</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Python]파이썬 크롤링(Crawling)/ 네이버, 멜론 웹크롤링</td>\n",
       "      <td>19.08.14</td>\n",
       "      <td>어준이 이야기</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[프로그래밍 입문반] 전수진님 : 웹 크롤링을 비전공자인 제가...</td>\n",
       "      <td>19.07.18</td>\n",
       "      <td>DS스쿨 공식블로그</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>파이썬으로 배우는 초보 웹크롤링...</td>\n",
       "      <td>18.02.03</td>\n",
       "      <td>etilelab</td>\n",
       "      <td>\\n\\n파이썬으로 배우는 초보 웹크롤링(Beautifulsoup라이브러리 설치)\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>웹 구조를 이해한 자의 웹크롤링은 데이터를 다루는...</td>\n",
       "      <td>19.07.15</td>\n",
       "      <td>패스트캠퍼스 : FAST CAMPUS</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>파이선 웹크롤링 초보 탈출기 1</td>\n",
       "      <td>18.11.17</td>\n",
       "      <td>아름다운 도시를 향한 발걸음</td>\n",
       "      <td>\\n\\n파이선 웹크롤링 초보 탈출기 1\\n\\n\\n 하루종일 컴퓨터 앞에 붙어서서 이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>뷰티풀 수프 이용해 보기 1부 : 웹 크롤링 개념</td>\n",
       "      <td>18.11.25</td>\n",
       "      <td>Data Miner</td>\n",
       "      <td>\\n\\n파이썬(Python) - Beautifulsoup 뷰티풀 수프 이용해 보기 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>웹크롤링 [금통위의사록 파이썬으로 다운받기]</td>\n",
       "      <td>19.07.13</td>\n",
       "      <td>Daikoku's blog</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[Python] 파이썬 웹 크롤링 #1. 네이버 실시간 검색어 가져오기</td>\n",
       "      <td>19.07.16</td>\n",
       "      <td>넬티아의 컴퓨터 블로그</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[파이썬으로 개발 시작하기 CAMP] - 파이썬 웹 크롤링</td>\n",
       "      <td>18.08.23</td>\n",
       "      <td>연시루 블로그</td>\n",
       "      <td>\\n\\n[파이썬으로 개발 시작하기 CAMP] - 파이썬 웹 크롤링\\n\\n\\n※ 패스...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[웹크롤링] 부동산 데이터 크롤링 : 트룰리아 (trulia.com)</td>\n",
       "      <td>19.07.10</td>\n",
       "      <td>웹 크롤링 &amp; 웹 스크래핑</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>웹 크롤링 &amp;gt; &amp;quot;파이썬으로 웹 크롤러 만들기&amp;quot;</td>\n",
       "      <td>19.05.04</td>\n",
       "      <td>카이사르의 블로그</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>크롤링은 이 책 하나면 충분하다, '파이썬으로 웹 크롤러...</td>\n",
       "      <td>19.06.09</td>\n",
       "      <td>Computer Repository</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>파이썬 웹 크롤링 [2편] with Selenium, Chrome</td>\n",
       "      <td>18.12.23</td>\n",
       "      <td>소소한 코딩교실</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>파이썬 kind공시 웹크롤링(1) Selenium을 이용한 방법</td>\n",
       "      <td>19.05.01</td>\n",
       "      <td>candycandy4785님의블로그</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>배우는 초보 웹크롤링(request요청, 영화차트크롤링-1)</td>\n",
       "      <td>18.02.08</td>\n",
       "      <td>etilelab</td>\n",
       "      <td>\\n\\n파이썬으로 배우는 초보 웹크롤링(request요청, 영화차트크롤링-1)\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Atom(아톰) 에디터로 웹 크롤링을 시작해보자</td>\n",
       "      <td>19.07.17</td>\n",
       "      <td>Somehn Industry</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[수강생인터뷰]R을 활용한 실전 웹크롤링_허인규님</td>\n",
       "      <td>18.12.19</td>\n",
       "      <td>패스트캠퍼스 : FAST CAMPUS</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>광고용으로 제작된 다중 VM 웹크롤링 젠서버 컴퓨터 입니다.</td>\n",
       "      <td>18.11.14</td>\n",
       "      <td>벡스컴</td>\n",
       "      <td>\\n\\n광고용으로 제작된 다중 VM 웹크롤링 젠서버 컴퓨터 입니다.\\n\\n\\n안녕하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[완친파 웹크롤러 대마왕편] 파이썬을 통한 크롤링</td>\n",
       "      <td>19.08.27</td>\n",
       "      <td>트롤의 일상</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[JAVA] JAVA로 웹 크롤링하기</td>\n",
       "      <td>19.06.14</td>\n",
       "      <td>Untitled</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>엑셀VBA를 이용한 웹 크롤링, 웹 데이터 수집의 기술로 배우자.</td>\n",
       "      <td>17.08.27</td>\n",
       "      <td>부산 캘리그라피 익사한생선</td>\n",
       "      <td>\\n\\n엑셀VBA를 이용한 웹 크롤링, 웹 데이터 수집의 기술로 배우자.\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>보기 2부 : 웹 크롤링 실습 : 프로그램에 HTML 가져오기</td>\n",
       "      <td>18.11.27</td>\n",
       "      <td>Data Miner</td>\n",
       "      <td>\\n\\n파이썬(Python) - Beautifulsoup 뷰티풀 수프 이용해 보기 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[웹크롤링] 웹크롤링이 항공, 호텔, 여행의 매출 증대에...</td>\n",
       "      <td>19.06.12</td>\n",
       "      <td>웹 크롤링 &amp; 웹 스크래핑</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>▣ 웹크롤링 / 스크래핑 프로그램 OUTWIT HUB 사용기</td>\n",
       "      <td>19.05.10</td>\n",
       "      <td>즐거움의 시작, 디바이스마트~♪</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[Python]데이터 도표를 읽어오기위한 웹 크롤링 방법 중...</td>\n",
       "      <td>19.01.12</td>\n",
       "      <td>블랙커피의 인생살이</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>파이썬 웹 크롤링 [1편] with Selenium, Chrome</td>\n",
       "      <td>18.10.17</td>\n",
       "      <td>소소한 코딩교실</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[Java] Jsoup를 이용한 간단 웹크롤링, 웹스크래핑</td>\n",
       "      <td>19.03.01</td>\n",
       "      <td>켈라네</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>웹 크롤링과 텍스트 분석, 네이버 영화 리뷰 어떻게 가져올까요?</td>\n",
       "      <td>18.04.22</td>\n",
       "      <td>커리어의 시작과 성장을, 러닝스푼즈에서.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>웹크롤링, 진작 제대로 배울 걸 그랬어요.</td>\n",
       "      <td>17.11.01</td>\n",
       "      <td>패스트캠퍼스 : FAST CAMPUS</td>\n",
       "      <td>\\n\\n웹크롤링, 진작 제대로 배울 걸 그랬어요.\\n\\n\\n\\n\\n1. 안녕하세요,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>배우는 초보 웹크롤링(Beautifulsoup, 영화차트크롤링-2)</td>\n",
       "      <td>18.02.14</td>\n",
       "      <td>etilelab</td>\n",
       "      <td>\\n\\n파이썬으로 배우는 초보 웹크롤링(Beautifulsoup, 영화차트크롤링-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>빅데이터 분석 마케팅_웹크롤링 및 워드 클라우드</td>\n",
       "      <td>19.05.12</td>\n",
       "      <td>안쌤의 올댓 포르투갈어</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>웹크롤링 엔진 아만다(Amanda)</td>\n",
       "      <td>19.07.02</td>\n",
       "      <td>Funny Life</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>rvest를 이용한 웹크롤링 in R</td>\n",
       "      <td>19.03.02</td>\n",
       "      <td>구만왕's 라이브러리</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>[Python] 네이버 블로그 Selenium 웹 크롤링</td>\n",
       "      <td>19.08.12</td>\n",
       "      <td>좀 더 나은 삶</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>웹 크롤링(Web Scraping)</td>\n",
       "      <td>18.04.17</td>\n",
       "      <td>하늘이 좋습니다 https://github.com/Jaekyu-Sim</td>\n",
       "      <td>\\n\\n웹 크롤링(Web Scraping)\\n\\n\\n웹 크롤링이란 웹 사이트에서 원...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>한입에 웹 크롤링</td>\n",
       "      <td>19.09.05</td>\n",
       "      <td>World.Peace.Idea</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>[웹크롤링] 1 , requests 와 BeautifulSoup</td>\n",
       "      <td>19.08.08</td>\n",
       "      <td>K4keye</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>웹 크롤링 상품정보수집, 상품자동등록</td>\n",
       "      <td>19.08.20</td>\n",
       "      <td>맞춤제작프로그램 이랑소프트</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>[Python/ 웹크롤링] 영화 IMAX 오픈 알리미 만들기 1</td>\n",
       "      <td>19.08.09</td>\n",
       "      <td>Information_Security '-'</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>C언어만 배운 개발자의 웹 크롤링(오픈마켓)</td>\n",
       "      <td>19.08.29</td>\n",
       "      <td>콩딱파파의 일상</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>로또 868회차 웹 크롤링</td>\n",
       "      <td>19.07.21</td>\n",
       "      <td>포림프노트</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>[에버오토] 웹 크롤링 - 부동산 사이트에서 지역별 중개사무소...</td>\n",
       "      <td>19.01.23</td>\n",
       "      <td>에버오토 (업무자동화의 모든 것)</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>[Python] 웹페이지 크롤링 후 Slack으로 공유</td>\n",
       "      <td>19.07.03</td>\n",
       "      <td>돈많은 백수를 꿈꾸는 개발자</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>[python] 웹 크롤링 예제</td>\n",
       "      <td>18.11.16</td>\n",
       "      <td>그런즉 선 줄로 생각하는 자는 넘어질까 조심하라.</td>\n",
       "      <td>\\n\\n[python] 웹 크롤링 예제\\n\\n\\n## 크롤링 : 거미줄처럼 연결된 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>웹크롤링 연습 1) 멜론차트 100위까지 출력하기</td>\n",
       "      <td>19.09.10</td>\n",
       "      <td>만들어가는 블로그</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>R : 간단한 웹 크롤링(Web Crawling) 실습</td>\n",
       "      <td>19.03.18</td>\n",
       "      <td>데이터 과학</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>[Android Kotlin] 대학교 학식 메뉴 '웹 크롤링' 해보기</td>\n",
       "      <td>19.07.18</td>\n",
       "      <td>취미 개발 BLOG</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>몰피곰의 책 이야기 - 한입에 웹 크롤링</td>\n",
       "      <td>19.01.18</td>\n",
       "      <td>몰피곰의 변화하는 블로그</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>[PYTHON] 웹 크롤링 소스코드 예제</td>\n",
       "      <td>19.04.19</td>\n",
       "      <td>-</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>자바를 이용한 웹 크롤링 만들기</td>\n",
       "      <td>19.08.18</td>\n",
       "      <td>개밥자의 블로그</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>[빅토리이야기] 텍스트마이닝/웹크롤링 실습후기</td>\n",
       "      <td>17.08.09</td>\n",
       "      <td>EUNJOYFUL.COM</td>\n",
       "      <td>\\n\\n[빅토리이야기] 텍스트마이닝/웹크롤링 실습후기\\n\\n\\n8/5 텍스트마이닝과...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>파이썬웹크롤링강의를 요청해주세요!</td>\n",
       "      <td>18.02.08</td>\n",
       "      <td>etilelab</td>\n",
       "      <td>\\n\\n파이썬웹크롤링강의를 요청해주세요!\\n\\n\\n파이썬웹크롤링을 진행하다가 막히는...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>[서평] 자바스크립트와 Node.js를 이용한 웹크롤링 테크닉</td>\n",
       "      <td>17.01.22</td>\n",
       "      <td>gameb</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>구글 &amp;amp; Bing 웹사이트 크롤링</td>\n",
       "      <td>09.11.20</td>\n",
       "      <td>OH MY CAT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>웹 크롤링 테크닉</td>\n",
       "      <td>17.03.24</td>\n",
       "      <td>마음이 한적한 곳으로...</td>\n",
       "      <td>\\n\\n웹 크롤링 테크닉\\n\\n\\n\\n지인의 소개로 순전히 \"재미\"를 위해서 읽기 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>가불관리 알파]인터넷에서 정보를 긁어오자! 웹 크롤링!(java편)</td>\n",
       "      <td>16.08.12</td>\n",
       "      <td>실시간 자동시급계산 앱 알파요</td>\n",
       "      <td>\\n\\n[출퇴근기록과 자동시급계산 및 가불관리 알파]인터넷에서 정보를 긁어오자! 웹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>쿠팡 웹 크롤링 맛보기</td>\n",
       "      <td>19.07.05</td>\n",
       "      <td>dailyCoding</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>웹사이트 무단 크롤링에 대한 (차)목의 부정경쟁행위 인정...</td>\n",
       "      <td>16.03.07</td>\n",
       "      <td>과학기술과 법 :: 실용적 법률 정보와 해결방안</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Node.js로 웹 크롤링하기</td>\n",
       "      <td>19.03.27</td>\n",
       "      <td>인프런 블로그</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>R에서 웹 크롤링! 코드 분석</td>\n",
       "      <td>15.09.05</td>\n",
       "      <td>어쩐지 오늘은</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title  postdate  \\\n",
       "0   [Python]파이썬 크롤링(Crawling)/ 네이버, 멜론 웹크롤링  19.08.14   \n",
       "1    [프로그래밍 입문반] 전수진님 : 웹 크롤링을 비전공자인 제가...   19.07.18   \n",
       "2                     파이썬으로 배우는 초보 웹크롤링...   18.02.03   \n",
       "3           웹 구조를 이해한 자의 웹크롤링은 데이터를 다루는...   19.07.15   \n",
       "4                         파이선 웹크롤링 초보 탈출기 1  18.11.17   \n",
       "..                                      ...       ...   \n",
       "89    가불관리 알파]인터넷에서 정보를 긁어오자! 웹 크롤링!(java편)  16.08.12   \n",
       "90                             쿠팡 웹 크롤링 맛보기  19.07.05   \n",
       "91      웹사이트 무단 크롤링에 대한 (차)목의 부정경쟁행위 인정...   16.03.07   \n",
       "92                         Node.js로 웹 크롤링하기  19.03.27   \n",
       "93                         R에서 웹 크롤링! 코드 분석  15.09.05   \n",
       "\n",
       "                   bloggername  \\\n",
       "0                      어준이 이야기   \n",
       "1                   DS스쿨 공식블로그   \n",
       "2                     etilelab   \n",
       "3         패스트캠퍼스 : FAST CAMPUS   \n",
       "4              아름다운 도시를 향한 발걸음   \n",
       "..                         ...   \n",
       "89            실시간 자동시급계산 앱 알파요   \n",
       "90                 dailyCoding   \n",
       "91  과학기술과 법 :: 실용적 법률 정보와 해결방안   \n",
       "92                     인프런 블로그   \n",
       "93                     어쩐지 오늘은   \n",
       "\n",
       "                                                 text  \n",
       "0                                                      \n",
       "1                                                      \n",
       "2   \\n\\n파이썬으로 배우는 초보 웹크롤링(Beautifulsoup라이브러리 설치)\\n...  \n",
       "3                                                      \n",
       "4   \\n\\n파이선 웹크롤링 초보 탈출기 1\\n\\n\\n 하루종일 컴퓨터 앞에 붙어서서 이...  \n",
       "..                                                ...  \n",
       "89  \\n\\n[출퇴근기록과 자동시급계산 및 가불관리 알파]인터넷에서 정보를 긁어오자! 웹...  \n",
       "90                                                     \n",
       "91                                                     \n",
       "92                                                     \n",
       "93                                                     \n",
       "\n",
       "[94 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
